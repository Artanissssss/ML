# 机器学习公式与概念速查表 📋

> 快速查找关键公式、算法和概念

---

## 📊 评估指标

### 分类指标

| 指标 | 公式 | 说明 |
|------|------|------|
| 准确率 (Accuracy) | `(TP + TN) / (TP + TN + FP + FN)` | 总体正确率 |
| 精确率 (Precision) | `TP / (TP + FP)` | 预测为正的准确性 |
| 召回率 (Recall) | `TP / (TP + FN)` | 找出正样本的能力 |
| F1分数 | `2 × (P × R) / (P + R)` | 精确率和召回率的调和平均 |
| 特异性 (Specificity) | `TN / (TN + FP)` | 识别负样本的能力 |

### 回归指标

| 指标 | 公式 | 说明 |
|------|------|------|
| MSE | `(1/n) Σ(yi - ŷi)²` | 均方误差 |
| RMSE | `√MSE` | 均方根误差 |
| MAE | `(1/n) Σ|yi - ŷi|` | 平均绝对误差 |
| R² | `1 - (SS_res / SS_tot)` | 决定系数 |

---

## 🎯 监督学习算法

### 线性回归

```
假设函数: h(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ = θᵀx

损失函数 (MSE): J(θ) = (1/2m) Σᵢ₌₁ᵐ (h(x⁽ⁱ⁾) - y⁽ⁱ⁾)²

梯度下降: θⱼ := θⱼ - α · (1/m) Σᵢ₌₁ᵐ (h(x⁽ⁱ⁾) - y⁽ⁱ⁾) · xⱼ⁽ⁱ⁾

正规方程: θ = (XᵀX)⁻¹Xᵀy
```

**何时使用**：
- ✅ 线性关系明显
- ✅ 需要解释性
- ❌ 特征数量 >> 样本数量（考虑正则化）

### 逻辑回归

```
Sigmoid函数: σ(z) = 1 / (1 + e⁻ᶻ)

假设函数: h(x) = σ(θᵀx) = 1 / (1 + e⁻θᵀˣ)

损失函数 (交叉熵): 
J(θ) = -(1/m) Σ [y⁽ⁱ⁾ log(h(x⁽ⁱ⁾)) + (1-y⁽ⁱ⁾) log(1-h(x⁽ⁱ⁾))]

预测: y = 1 if h(x) ≥ 0.5, else y = 0
```

**何时使用**：
- ✅ 二分类问题
- ✅ 需要概率输出
- ✅ 特征与对数几率呈线性关系

### 决策树

**分裂准则**：

**信息增益 (ID3)**：
```
Entropy(S) = -Σ pᵢ log₂(pᵢ)
Gain(S, A) = Entropy(S) - Σ (|Sᵥ|/|S|) · Entropy(Sᵥ)
```

**信息增益率 (C4.5)**：
```
GainRatio(S, A) = Gain(S, A) / SplitInfo(S, A)
SplitInfo(S, A) = -Σ (|Sᵥ|/|S|) log₂(|Sᵥ|/|S|)
```

**基尼不纯度 (CART)**：
```
Gini(S) = 1 - Σ pᵢ²
```

**何时使用**：
- ✅ 需要可解释性
- ✅ 特征混合（数值+类别）
- ❌ 容易过拟合（需要剪枝或集成）

### 支持向量机 (SVM)

```
优化目标: min (1/2)||w||² + C Σ ξᵢ
约束条件: yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0

核函数:
- 线性核: K(x, x') = xᵀx'
- 多项式核: K(x, x') = (γxᵀx' + r)ᵈ
- RBF核: K(x, x') = exp(-γ||x - x'||²)
- Sigmoid核: K(x, x') = tanh(γxᵀx' + r)
```

**何时使用**：
- ✅ 高维数据
- ✅ 需要非线性决策边界（使用核函数）
- ❌ 大规模数据（训练慢）

### K近邻 (KNN)

```
距离度量:
- 欧式距离: d(x, x') = √(Σ(xᵢ - x'ᵢ)²)
- 曼哈顿距离: d(x, x') = Σ|xᵢ - x'ᵢ|
- 闵可夫斯基距离: d(x, x') = (Σ|xᵢ - x'ᵢ|ᵖ)^(1/p)

分类: majority vote of k nearest neighbors
回归: average of k nearest neighbors
```

**何时使用**：
- ✅ 数据量不太大
- ✅ 不需要训练（懒惰学习）
- ❌ 高维数据（维度灾难）

---

## 🧠 神经网络

### 激活函数对比

| 函数 | 公式 | 导数 | 优点 | 缺点 |
|------|------|------|------|------|
| Sigmoid | `σ(x) = 1/(1+e⁻ˣ)` | `σ(x)(1-σ(x))` | 平滑，输出[0,1] | 梯度消失 |
| Tanh | `tanh(x)` | `1 - tanh²(x)` | 输出[-1,1]，零中心 | 梯度消失 |
| ReLU | `max(0, x)` | `1 if x>0 else 0` | 计算快，缓解梯度消失 | 神经元死亡 |
| Leaky ReLU | `max(αx, x)` | `α if x<0 else 1` | 避免神经元死亡 | 需调参α |
| ELU | `x if x>0 else α(eˣ-1)` | `1 if x>0 else f(x)+α` | 负值有输出 | 计算复杂 |
| Softmax | `eˣⁱ/Σeˣʲ` | 复杂 | 多分类概率 | 仅输出层 |

### 反向传播

```
前向传播:
z⁽ˡ⁾ = W⁽ˡ⁾a⁽ˡ⁻¹⁾ + b⁽ˡ⁾
a⁽ˡ⁾ = g(z⁽ˡ⁾)

反向传播:
δ⁽ᴸ⁾ = ∇ₐL ⊙ g'(z⁽ᴸ⁾)
δ⁽ˡ⁾ = ((W⁽ˡ⁺¹⁾)ᵀδ⁽ˡ⁺¹⁾) ⊙ g'(z⁽ˡ⁾)

梯度:
∂L/∂W⁽ˡ⁾ = δ⁽ˡ⁾(a⁽ˡ⁻¹⁾)ᵀ
∂L/∂b⁽ˡ⁾ = δ⁽ˡ⁾
```

### 优化器对比

| 优化器 | 更新规则 | 特点 |
|--------|---------|------|
| SGD | `θ := θ - α∇J(θ)` | 简单，但收敛慢 |
| Momentum | `v := βv - α∇J`<br>`θ := θ + v` | 加速收敛，减少震荡 |
| AdaGrad | `θ := θ - α/(√G+ε) ∇J` | 自适应学习率 |
| RMSprop | `E[g²] := βE[g²] + (1-β)g²`<br>`θ := θ - α/√(E[g²]+ε) g`<br>(g = 梯度) | 解决AdaGrad学习率递减 |
| Adam | 结合Momentum和RMSprop | **最常用**，效果好 |

---

## 🏗️ 深度学习

### CNN核心操作

**卷积**：
```
输出尺寸 = ⌊(W - F + 2P) / S⌋ + 1

W: 输入尺寸
F: 卷积核尺寸
P: 填充 (Padding)
S: 步长 (Stride)
```

**池化**：
```
Max Pooling: 取窗口内最大值
Average Pooling: 取窗口内平均值
```

**参数量计算**：
```
卷积层参数 = (F × F × C_in + 1) × C_out
全连接层参数 = (输入维度 + 1) × 输出维度
```

### RNN与LSTM

**标准RNN**：
```
hₜ = tanh(Wₕₕhₜ₋₁ + Wₓₕxₜ + bₕ)
yₜ = Wₕᵧhₜ + bᵧ
```

**LSTM单元**：
```
遗忘门: fₜ = σ(Wf·[hₜ₋₁, xₜ] + bf)
输入门: iₜ = σ(Wi·[hₜ₋₁, xₜ] + bi)
输出门: oₜ = σ(Wo·[hₜ₋₁, xₜ] + bo)
候选值: C̃ₜ = tanh(Wc·[hₜ₋₁, xₜ] + bc)
细胞状态: Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ
隐藏状态: hₜ = oₜ ⊙ tanh(Cₜ)
```

### 正则化技术

| 技术 | 原理 | 使用场景 |
|------|------|---------|
| L1正则 | 损失 + λΣ\|w\| | 特征选择，稀疏解 |
| L2正则 | 损失 + λΣw² | 防止过拟合 |
| Dropout | 随机失活神经元 | 训练时p概率丢弃 |
| Batch Norm | 归一化每批数据 | 加速训练，稳定 |
| Early Stopping | 监控验证集 | 防止过拟合 |
| Data Augmentation | 增加训练数据 | 图像、文本任务 |

---

## 📦 聚类算法

### K-means

```
算法流程:
1. 随机初始化K个中心点
2. 分配：每个点归到最近的中心
3. 更新：重新计算每个簇的中心
4. 重复2-3直到收敛

目标函数: J = Σₖ Σₓ∈Cₖ ||x - μₖ||²

K值选择：肘部法则 (Elbow Method)
```

**优点**：
- 简单高效
- 适合球形簇

**缺点**：
- 需要预设K
- 对初始值敏感
- 对异常值敏感

### DBSCAN

```
参数:
- ε: 邻域半径
- MinPts: 核心点的最小邻域点数

点的类型:
- 核心点: |N(x)| ≥ MinPts
- 边界点: 不是核心点但在核心点邻域内
- 噪声点: 既非核心点也非边界点
```

**优点**：
- 不需要预设簇数
- 能发现任意形状的簇
- 能识别噪声

**缺点**：
- 参数敏感
- 密度不均时效果差

### 层次聚类

```
凝聚式 (自下而上):
1. 每个点作为一个簇
2. 合并最近的两个簇
3. 重复直到只剩一个簇

链接方法:
- 单链接: min{d(x,y) | x∈Ci, y∈Cj}
- 全链接: max{d(x,y) | x∈Ci, y∈Cj}
- 平均链接: avg{d(x,y) | x∈Ci, y∈Cj}
- Ward: 最小化方差
```

### 聚类评估

**轮廓系数 (Silhouette)**：
```
s(i) = (b(i) - a(i)) / max{a(i), b(i)}

a(i): 点i到同簇其他点的平均距离
b(i): 点i到最近其他簇的平均距离

范围: [-1, 1], 越接近1越好
```

---

## 📉 降维技术

### PCA (主成分分析)

```
步骤:
1. 数据中心化: X̃ = X - mean(X)
2. 计算协方差矩阵: Σ = (1/n)X̃ᵀX̃
3. 特征值分解: Σ = VΛVᵀ
4. 选择前k个最大特征值对应的特征向量
5. 投影: Z = X̃Vₖ

方差解释比例 = λₖ / Σᵢλᵢ
```

**选择主成分个数**：
- 累计方差贡献率 ≥ 85%~95%
- 陡坡检验 (Scree Plot)

**PCA vs LDA**：
| | PCA | LDA |
|---|-----|-----|
| 类型 | 无监督 | 监督 |
| 目标 | 最大方差 | 最大类间距离 |
| 主成分数 | ≤ 特征数 | ≤ 类别数-1 |

### t-SNE

```
高维相似度 (条件概率p(j|i)): 
pⱼ|ᵢ = exp(-||xᵢ-xⱼ||²/2σᵢ²) / Σₖ≠ᵢ exp(-||xᵢ-xₖ||²/2σᵢ²)

低维相似度: 
qᵢⱼ = (1 + ||yᵢ-yⱼ||²)⁻¹ / Σₖ≠ₗ(1 + ||yₖ-yₗ||²)⁻¹

KL散度: KL(P||Q) = Σᵢ Σⱼ pᵢⱼ log(pᵢⱼ/qᵢⱼ)
```

**参数**：
- Perplexity (困惑度): 5-50，反映邻域大小
- 迭代次数: 通常1000+

**注意**：
- ⚠️ 主要用于可视化，不用于降维后建模
- ⚠️ 距离信息不完全保留
- ⚠️ 计算复杂度高

---

## 🎲 密度估计

### 核密度估计 (KDE)

```
f̂(x) = (1/nh) Σᵢ₌₁ⁿ K((x - xᵢ)/h)

K: 核函数
h: 带宽 (bandwidth)

常用核函数:
- 高斯核: K(u) = (1/√2π) exp(-u²/2)
- Epanechnikov核: K(u) = (3/4)(1-u²) if |u|≤1
```

### 高斯混合模型 (GMM)

```
概率密度: p(x) = Σₖ πₖ N(x|μₖ, Σₖ)

EM算法:
E步: γᵢₖ = πₖN(xᵢ|μₖ,Σₖ) / ΣⱼπⱼN(xᵢ|μⱼ,Σⱼ)
M步: 
  πₖ = (1/N)Σᵢγᵢₖ
  μₖ = Σᵢγᵢₖxᵢ / Σᵢγᵢₖ
  Σₖ = Σᵢγᵢₖ(xᵢ-μₖ)(xᵢ-μₖ)ᵀ / Σᵢγᵢₖ
```

---

## 🔧 模型选择

### 交叉验证

**K折交叉验证**：
```
1. 将数据分成K份
2. 每次用K-1份训练，1份验证
3. 重复K次，每份都做一次验证集
4. 平均K次结果

常用: K=5或K=10
```

**留一法 (LOO)**：
```
K = N（样本数）
计算量大但方差小
```

### 偏差-方差权衡

```
总误差 = Bias² + Variance + 不可约误差

Bias (偏差): 模型预测的期望与真实值的偏离
Variance (方差): 模型在不同训练集上的波动

高偏差 → 欠拟合 → 增加模型复杂度
高方差 → 过拟合 → 正则化、更多数据
```

---

## 🎯 算法选择指南

### 监督学习

**问题类型** → **推荐算法**

**回归问题**：
- 线性关系 → 线性回归
- 非线性 → 决策树、随机森林、神经网络
- 高维稀疏 → Lasso回归

**二分类**：
- 线性可分 → 逻辑回归、线性SVM
- 非线性 → SVM+核函数、神经网络
- 需要概率 → 逻辑回归、神经网络
- 需要解释 → 决策树、逻辑回归

**多分类**：
- 类别少 → Softmax回归、多分类SVM
- 类别多 → 神经网络、随机森林

**样本特点** → **算法选择**：
- 样本少 → KNN、朴素贝叶斯、SVM
- 样本大 → 神经网络、线性模型、XGBoost
- 高维 → SVM、L1正则化
- 不平衡 → 调整权重、SMOTE、集成方法

### 无监督学习

**聚类**：
- 球形簇 → K-means
- 任意形状 → DBSCAN
- 层次结构 → 层次聚类
- 软聚类 → GMM

**降维**：
- 线性降维 → PCA
- 可视化 → t-SNE
- 监督降维 → LDA
- 特征提取 → 自编码器

---

## ⚡ 快速记忆技巧

### 缩写词记忆

**MNIST**: Modified National Institute of Standards and Technology
**SGD**: Stochastic Gradient Descent
**ReLU**: Rectified Linear Unit
**LSTM**: Long Short-Term Memory
**CNN**: Convolutional Neural Network
**RNN**: Recurrent Neural Network
**GAN**: Generative Adversarial Network
**PCA**: Principal Component Analysis
**LDA**: Linear Discriminant Analysis
**SVM**: Support Vector Machine
**KNN**: K-Nearest Neighbors
**EM**: Expectation-Maximization

### 对比记忆

**监督 vs 无监督**：
- 监督：有标签 (分类、回归)
- 无监督：无标签 (聚类、降维)

**生成 vs 判别**：
- 生成：学习P(X,Y)，如朴素贝叶斯
- 判别：学习P(Y|X)，如逻辑回归

**参数 vs 非参数**：
- 参数：固定参数数量，如线性回归
- 非参数：参数随数据增长，如KNN

---

## 📱 移动端友好版

**考试时快速查找**：
1. Ctrl+F 搜索关键词
2. 点击目录快速跳转
3. 重点公式已高亮

**建议**：
- 打印此文档随身携带
- 或保存为PDF在手机查看
- 考前30分钟快速浏览

---

*Good luck! 祝考试顺利！* 🍀
