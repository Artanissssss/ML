# 机器学习期末复习计划与自测题 📝

## 📅 7天复习计划

### Day 1: 基础概念与统计学习
- [ ] **上午** (3小时)
  - [ ] 复习 `1-Introduction.pptx`
  - [ ] 理解机器学习基本概念
  - [ ] 掌握学习类型的区别
  - [ ] 理解过拟合和欠拟合

- [ ] **下午** (3小时)
  - [ ] 复习 `2-Statistical Learning.pptx`
  - [ ] 理解偏差-方差权衡
  - [ ] 掌握损失函数和风险
  - [ ] 学习正则化方法
  - [ ] 练习交叉验证

- [ ] **晚上** (2小时)
  - [ ] 完成第一天自测题
  - [ ] 总结重点难点
  - [ ] 记忆关键公式

---

### Day 2: 监督学习 (线性模型)
- [ ] **上午** (3小时)
  - [ ] 复习 `3-Supervised Learning.pptx` 第一部分
  - [ ] 线性回归原理和推导
  - [ ] 梯度下降算法
  - [ ] 正规方程

- [ ] **下午** (3小时)
  - [ ] 逻辑回归详解
  - [ ] Sigmoid函数性质
  - [ ] 交叉熵损失函数
  - [ ] 多分类问题 (Softmax)

- [ ] **晚上** (2小时)
  - [ ] 手推线性回归公式
  - [ ] 练习逻辑回归计算
  - [ ] 完成自测题

---

### Day 3: 监督学习 (树模型与SVM)
- [ ] **上午** (3小时)
  - [ ] 决策树算法
  - [ ] 信息增益、基尼系数
  - [ ] 剪枝策略
  - [ ] 集成学习 (Bagging, Boosting)

- [ ] **下午** (3小时)
  - [ ] 支持向量机 (SVM)
  - [ ] 最大间隔原理
  - [ ] 核技巧
  - [ ] 软间隔和硬间隔

- [ ] **晚上** (2小时)
  - [ ] K近邻算法
  - [ ] 朴素贝叶斯
  - [ ] 完成自测题

---

### Day 4: 神经网络
- [ ] **上午** (3小时)
  - [ ] 复习 `4-Neural Networks.pptx`
  - [ ] 感知机模型
  - [ ] 多层感知机
  - [ ] 各种激活函数

- [ ] **下午** (3小时)
  - [ ] **重点：反向传播算法**
  - [ ] 链式法则
  - [ ] 梯度下降变体
  - [ ] 优化器对比 (SGD, Adam等)

- [ ] **晚上** (2小时)
  - [ ] 手推反向传播
  - [ ] 理解梯度消失/爆炸
  - [ ] Dropout和BatchNorm
  - [ ] 完成自测题

---

### Day 5: 深度学习
- [ ] **上午** (3小时)
  - [ ] 复习 `5-Deep Learning.pdf`
  - [ ] CNN架构详解
  - [ ] 卷积层和池化层
  - [ ] 经典CNN模型

- [ ] **下午** (3小时)
  - [ ] RNN和序列模型
  - [ ] LSTM和GRU
  - [ ] 注意力机制
  - [ ] Transformer简介

- [ ] **晚上** (2小时)
  - [ ] 其他模型：VAE, GAN
  - [ ] 迁移学习
  - [ ] 完成自测题

---

### Day 6: 无监督学习
- [ ] **上午** (3小时)
  - [ ] 复习 `6-Clustering.pptx`
  - [ ] K-means算法详解
  - [ ] DBSCAN原理
  - [ ] 层次聚类
  - [ ] 高斯混合模型

- [ ] **下午** (3小时)
  - [ ] 复习 `8-Dimensionality Reduction.pptx`
  - [ ] **重点：PCA推导**
  - [ ] LDA与PCA区别
  - [ ] t-SNE原理
  - [ ] 其他降维方法

- [ ] **晚上** (2小时)
  - [ ] 复习 `7-Density Estimation.pptx`
  - [ ] 核密度估计
  - [ ] EM算法
  - [ ] 完成自测题

---

### Day 7: 综合复习与模拟
- [ ] **上午** (3小时)
  - [ ] 快速浏览所有PPT
  - [ ] 重点回顾标记的难点
  - [ ] 复习错题

- [ ] **下午** (3小时)
  - [ ] 完成综合自测题
  - [ ] 模拟考试环境
  - [ ] 计时练习

- [ ] **晚上** (2小时)
  - [ ] 默写核心公式
  - [ ] 整理知识框架
  - [ ] 早休息，准备考试

---

## 🧪 自测题

### 第一部分：选择题

#### 1. 基础概念

**Q1.1** 以下哪个不是监督学习任务？
- A. 垃圾邮件分类
- B. 房价预测
- C. 客户分群
- D. 手写数字识别

<details>
<summary>答案</summary>
C. 客户分群是聚类任务，属于无监督学习
</details>

**Q1.2** 当模型在训练集上表现很好，但在测试集上表现很差时，这是什么问题？
- A. 欠拟合
- B. 过拟合
- C. 偏差过高
- D. 学习率过大

<details>
<summary>答案</summary>
B. 过拟合 - 模型过度学习了训练数据的特征，泛化能力差
</details>

**Q1.3** 增加正则化参数λ会导致什么？
- A. 增加模型复杂度
- B. 减少偏差
- C. 增加方差
- D. 减少方差

<details>
<summary>答案</summary>
D. 减少方差 - 正则化限制模型复杂度，降低过拟合风险
</details>

#### 2. 监督学习

**Q2.1** 在逻辑回归中，当σ(z) = 0.7时，z的值大约是多少？
- A. -0.85
- B. 0
- C. 0.85
- D. 1.4

<details>
<summary>答案</summary>
C. 0.85 - 由σ(z) = 1/(1+e^(-z)) = 0.7，可得z ≈ 0.85
</details>

**Q2.2** 决策树使用信息增益作为分裂准则，其基础是哪个概念？
- A. 方差
- B. 熵
- C. 均方误差
- D. 似然

<details>
<summary>答案</summary>
B. 熵 - 信息增益 = 分裂前的熵 - 分裂后的加权熵
</details>

**Q2.3** SVM中的核技巧主要解决什么问题？
- A. 加快训练速度
- B. 处理非线性可分问题
- C. 减少特征数量
- D. 防止过拟合

<details>
<summary>答案</summary>
B. 处理非线性可分问题 - 将数据映射到高维空间使其线性可分
</details>

#### 3. 神经网络

**Q3.1** 为什么ReLU比Sigmoid更常用？
- A. ReLU可以输出负值
- B. ReLU计算更简单且缓解梯度消失
- C. ReLU输出范围更广
- D. ReLU总是可导的

<details>
<summary>答案</summary>
B. ReLU计算简单(max运算)，且在正区间梯度恒为1，缓解梯度消失
</details>

**Q3.2** Dropout在训练和测试时的行为有何不同？
- A. 训练时使用，测试时不使用
- B. 测试时使用，训练时不使用
- C. 都使用，但概率不同
- D. 没有区别

<details>
<summary>答案</summary>
A. Dropout只在训练时随机失活神经元，测试时使用全部神经元
</details>

**Q3.3** Adam优化器结合了哪两种方法的优点？
- A. SGD和BGD
- B. Momentum和RMSprop
- C. AdaGrad和Adadelta
- D. NAG和AdaMax

<details>
<summary>答案</summary>
B. Momentum（动量）和RMSprop（自适应学习率）
</details>

#### 4. 深度学习

**Q4.1** 在CNN中，池化层的主要作用是什么？
- A. 增加特征数量
- B. 减少参数和计算量
- C. 增加非线性
- D. 防止梯度消失

<details>
<summary>答案</summary>
B. 减少空间维度，降低参数量和计算量，同时提供平移不变性
</details>

**Q4.2** LSTM相比标准RNN的主要优势是什么？
- A. 计算速度更快
- B. 参数更少
- C. 能更好地处理长期依赖
- D. 不需要激活函数

<details>
<summary>答案</summary>
C. LSTM通过门控机制和细胞状态，能够更好地捕捉长期依赖关系
</details>

#### 5. 聚类

**Q5.1** K-means对什么最敏感？
- A. 特征缩放
- B. 初始中心点选择
- C. 数据分布
- D. 以上都是

<details>
<summary>答案</summary>
D. 以上都是 - K-means对初始值、特征尺度和簇形状都很敏感
</details>

**Q5.2** DBSCAN相比K-means的优势是什么？
- A. 不需要预设簇数
- B. 可以发现任意形状的簇
- C. 可以识别噪声点
- D. 以上都是

<details>
<summary>答案</summary>
D. 以上都是
</details>

#### 6. 降维

**Q6.1** PCA寻找的主成分方向是什么？
- A. 数据方差最小的方向
- B. 数据方差最大的方向
- C. 类间距离最大的方向
- D. 数据密度最大的方向

<details>
<summary>答案</summary>
B. 数据方差最大的方向 - PCA寻找保留最多信息（方差）的方向
</details>

**Q6.2** t-SNE主要用于什么？
- A. 特征提取用于分类
- B. 数据压缩
- C. 高维数据可视化
- D. 加速模型训练

<details>
<summary>答案</summary>
C. 高维数据可视化 - t-SNE擅长将高维数据降到2-3维用于可视化
</details>

---

### 第二部分：简答题

**Q7** 解释偏差-方差权衡（Bias-Variance Tradeoff）

<details>
<summary>参考答案</summary>

模型的总误差可以分解为：
- **偏差（Bias）**：模型预测的期望值与真实值之间的差距
  - 高偏差 → 欠拟合 → 模型过于简单
  
- **方差（Variance）**：模型在不同训练集上预测的波动程度
  - 高方差 → 过拟合 → 模型过于复杂

- **权衡**：
  - 降低偏差通常会增加方差（增加模型复杂度）
  - 降低方差通常会增加偏差（简化模型或正则化）
  - 目标是找到两者的最优平衡点

总误差 = Bias² + Variance + 不可约误差

</details>

**Q8** 比较Bagging和Boosting的区别

<details>
<summary>参考答案</summary>

| 特征 | Bagging | Boosting |
|------|---------|----------|
| **训练方式** | 并行训练多个独立模型 | 串行训练，后续模型依赖前面的 |
| **采样方式** | Bootstrap采样（有放回） | 调整样本权重 |
| **模型关系** | 独立 | 相互依赖 |
| **目标** | 减少方差 | 减少偏差和方差 |
| **代表算法** | Random Forest | AdaBoost, XGBoost |
| **过拟合风险** | 低 | 相对较高 |
| **并行性** | 易并行 | 难并行 |

**Bagging**：通过平均多个模型减少方差，适合高方差模型（如决策树）
**Boosting**：通过加权组合逐步提升性能，关注难分样本

</details>

**Q9** 解释反向传播算法的原理

<details>
<summary>参考答案</summary>

反向传播（Backpropagation）是训练神经网络的核心算法：

**基本思想**：
利用链式法则，从输出层向输入层逐层计算损失函数对每个参数的梯度。

**步骤**：
1. **前向传播**：计算每层的输出和最终损失
   ```
   z[l] = W[l]·a[l-1] + b[l]
   a[l] = g(z[l])
   ```

2. **反向传播**：从输出层开始计算梯度
   ```
   δ[L] = ∇L ⊙ g'(z[L])        # 输出层
   δ[l] = (W[l+1])ᵀδ[l+1] ⊙ g'(z[l])  # 隐藏层
   ```

3. **计算参数梯度**：
   ```
   ∂L/∂W[l] = δ[l] · (a[l-1])ᵀ
   ∂L/∂b[l] = δ[l]
   ```

4. **更新参数**：
   ```
   W := W - α·∂L/∂W
   b := b - α·∂L/∂b
   ```

**关键**：链式法则使得梯度可以高效地从后向前传播

</details>

**Q10** 说明PCA的步骤和数学原理

<details>
<summary>参考答案</summary>

**主成分分析（PCA）步骤**：

1. **数据标准化**
   ```
   X̃ = (X - μ) / σ
   ```

2. **计算协方差矩阵**
   ```
   Σ = (1/n)X̃ᵀX̃
   ```

3. **特征值分解**
   ```
   Σv = λv
   ```
   求解特征值λ和特征向量v

4. **选择主成分**
   - 按特征值从大到小排序
   - 选择前k个特征向量构成投影矩阵Vₖ
   - 通常选择累计方差贡献率≥85-95%

5. **投影数据**
   ```
   Z = X̃Vₖ
   ```

**数学原理**：
- PCA寻找数据方差最大的方向
- 第一主成分是方差最大的方向
- 后续主成分与前面正交，且方差次大
- 等价于最小化重构误差

**方差贡献率**：
```
λₖ / Σᵢλᵢ
```

</details>

---

### 第三部分：计算题

**Q11** 给定训练数据，使用梯度下降更新逻辑回归参数

数据：x = [1, 2], y = 1
当前参数：θ = [0, 0]
学习率：α = 0.1

计算：
1. h(x) = σ(θᵀx) = ?
2. 损失 L = ?
3. 更新后的θ = ?

<details>
<summary>解答</summary>

1. **计算预测值**：
   ```
   z = θᵀx = 0×1 + 0×2 = 0
   h(x) = σ(0) = 1/(1+e⁰) = 0.5
   ```

2. **计算损失**（单样本交叉熵）：
   ```
   L = -[y·log(h) + (1-y)·log(1-h)]
     = -[1·log(0.5) + 0·log(0.5)]
     = -log(0.5) = 0.693
   ```

3. **计算梯度**：
   ```
   ∂L/∂θ = (h(x) - y)·x
         = (0.5 - 1)·[1, 2]
         = [-0.5, -1.0]
   ```

4. **更新参数**：
   ```
   θ := θ - α·∂L/∂θ
   θ₀ := 0 - 0.1×(-0.5) = 0.05
   θ₁ := 0 - 0.1×(-1.0) = 0.1
   
   更新后：θ = [0.05, 0.1]
   ```

</details>

**Q12** 计算决策树的信息增益

数据集S有10个样本：6个正例，4个负例
特征A将数据分为两组：
- A=0: 2个正例，3个负例
- A=1: 4个正例，1个负例

计算特征A的信息增益。

<details>
<summary>解答</summary>

1. **计算原始熵**：
   ```
   Entropy(S) = -Σ pᵢ log₂(pᵢ)
              = -(6/10)log₂(6/10) - (4/10)log₂(4/10)
              = -0.6×(-0.737) - 0.4×(-1.322)
              = 0.442 + 0.529
              = 0.971
   ```

2. **计算分裂后的熵**：
   ```
   Entropy(A=0) = -(2/5)log₂(2/5) - (3/5)log₂(3/5)
                = 0.971
   
   Entropy(A=1) = -(4/5)log₂(4/5) - (1/5)log₂(1/5)
                = 0.722
   ```

3. **计算加权平均熵**：
   ```
   Entropy_after = (5/10)×0.971 + (5/10)×0.722
                 = 0.486 + 0.361
                 = 0.847
   ```

4. **计算信息增益**：
   ```
   Gain(S, A) = Entropy(S) - Entropy_after
              = 0.971 - 0.847
              = 0.124
   ```

**答案**：信息增益为 0.124

</details>

**Q13** PCA主成分选择

协方差矩阵的特征值为：λ₁=50, λ₂=30, λ₃=15, λ₄=5

1. 计算每个主成分的方差贡献率
2. 如果要保留95%的方差，应选择几个主成分？

<details>
<summary>解答</summary>

1. **计算总方差**：
   ```
   总方差 = 50 + 30 + 15 + 5 = 100
   ```

2. **各主成分方差贡献率**：
   ```
   PC1: 50/100 = 50%
   PC2: 30/100 = 30%
   PC3: 15/100 = 15%
   PC4: 5/100 = 5%
   ```

3. **累计方差贡献率**：
   ```
   PC1: 50%
   PC1+PC2: 80%
   PC1+PC2+PC3: 95%
   PC1+PC2+PC3+PC4: 100%
   ```

**答案**：
- 各主成分贡献率：50%, 30%, 15%, 5%
- 要保留95%的方差，需要选择**前3个主成分**

</details>

---

### 第四部分：综合应用题

**Q14** 模型选择与评估

你需要为一个医疗诊断系统选择分类器，数据特点如下：
- 1000个样本，100个特征
- 类别不平衡：阳性50例，阴性950例
- 要求：高召回率（不能漏诊），可接受较低精确率

问题：
1. 应该选择什么评估指标？
2. 推荐使用哪种算法？为什么？
3. 如何处理类别不平衡问题？

<details>
<summary>参考答案</summary>

**1. 评估指标**：
- **主要指标**：召回率（Recall）
  - 原因：医疗诊断中，漏诊（假阴性）的代价很高
  - Recall = TP/(TP+FN)，衡量找出所有阳性的能力
  
- **辅助指标**：
  - F1-score（平衡精确率和召回率）
  - PR曲线（Precision-Recall curve）
  - 不推荐使用准确率（Accuracy），因为类别不平衡

**2. 推荐算法**：

**首选**：
- **逻辑回归** + 类别权重调整
  - 可解释性强（医疗需要）
  - 输出概率，可调整阈值提高召回率
  - 支持类别权重设置

- **随机森林** + 类别权重
  - 对不平衡数据鲁棒
  - 特征重要性分析
  - 不易过拟合

**也可考虑**：
- SVM + 类别权重
- XGBoost + scale_pos_weight参数

**不推荐**：
- KNN（类别不平衡时效果差）
- 单棵决策树（容易过拟合）

**3. 处理类别不平衡**：

方法1：**重采样**
```python
# 过采样少数类（SMOTE）
# 欠采样多数类
# 组合采样
```

方法2：**类别权重**
```python
# 为少数类设置更高权重
class_weight = {0: 1, 1: 19}  # 950:50 ≈ 19:1
```

方法3：**调整阈值**
```python
# 降低阳性判定阈值
if prob > 0.3:  # 而不是默认的0.5
    predict = 1
```

方法4：**集成方法**
```python
# BalancedRandomForest
# EasyEnsemble
```

方法5：**评估指标优化**
```python
# 直接优化召回率
# 使用F2-score（更重视召回率）
```

**推荐组合方案**：
```
逻辑回归 + SMOTE过采样 + 类别权重 + 阈值调整
或
XGBoost + scale_pos_weight + 阈值调整
```

</details>

**Q15** 神经网络设计

设计一个神经网络用于MNIST手写数字识别（28×28灰度图像，10个类别）

问题：
1. 设计网络架构
2. 选择激活函数、损失函数、优化器
3. 如何防止过拟合？
4. 估算第一个全连接层的参数量

<details>
<summary>参考答案</summary>

**1. 网络架构设计**：

**方案A：简单MLP**
```
输入层：28×28 = 784维
隐藏层1：128个神经元 + ReLU
隐藏层2：64个神经元 + ReLU
输出层：10个神经元 + Softmax
```

**方案B：CNN（推荐）**
```
输入：28×28×1
Conv1: 32个3×3卷积核 + ReLU
MaxPool1: 2×2
Conv2: 64个3×3卷积核 + ReLU
MaxPool2: 2×2
Flatten
FC1: 128个神经元 + ReLU
Dropout: 0.5
FC2: 10个神经元 + Softmax
```

**2. 函数选择**：

- **激活函数**：
  - 隐藏层：ReLU（计算快，缓解梯度消失）
  - 输出层：Softmax（多分类概率输出）

- **损失函数**：
  - 交叉熵（Categorical Cross-Entropy）
  - 公式：L = -Σᵢ yᵢ log(ŷᵢ)

- **优化器**：
  - Adam（自适应学习率，效果好）
  - 学习率：0.001（默认）
  - 或SGD + Momentum（更稳定）

**3. 防止过拟合**：

```python
# 方法1：Dropout
Dropout(0.5)  # 随机失活50%神经元

# 方法2：L2正则化
kernel_regularizer=l2(0.01)

# 方法3：Early Stopping
EarlyStopping(patience=5)

# 方法4：Batch Normalization
BatchNormalization()

# 方法5：数据增强
ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1
)

# 方法6：减少网络复杂度
# 减少层数或神经元数量
```

**4. 参数量计算**：

**MLP方案**：
```
输入到第一隐藏层：
参数 = (784 + 1) × 128
     = 785 × 128
     = 100,480个参数
(+1是偏置项)
```

**CNN方案第一层**：
```
Conv1层：
参数 = (3×3×1 + 1) × 32
     = 10 × 32
     = 320个参数
(远少于全连接层！)
```

**完整MLP参数量**：
```
输入→隐藏1：784×128 + 128 = 100,480
隐藏1→隐藏2：128×64 + 64 = 8,256
隐藏2→输出：64×10 + 10 = 650
总计：109,386个参数
```

**推荐配置**：
```python
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

</details>

---

## 🎯 学习检查点

完成以上自测题后，检查你的掌握程度：

- [ ] **基础概念** (90%+正确率)
  - [ ] 监督vs无监督学习
  - [ ] 过拟合vs欠拟合
  - [ ] 偏差-方差权衡

- [ ] **监督学习** (85%+正确率)
  - [ ] 线性回归和逻辑回归
  - [ ] 决策树和集成方法
  - [ ] SVM和核技巧

- [ ] **神经网络** (80%+正确率)
  - [ ] 反向传播算法
  - [ ] 激活函数和优化器
  - [ ] 正则化技术

- [ ] **深度学习** (75%+正确率)
  - [ ] CNN架构
  - [ ] RNN和LSTM
  - [ ] 模型设计原则

- [ ] **无监督学习** (80%+正确率)
  - [ ] 聚类算法
  - [ ] PCA降维
  - [ ] 密度估计

---

## 💡 答题技巧

### 选择题策略
1. 先做有把握的题
2. 排除法：先排除明显错误选项
3. 注意关键词："最"、"主要"、"不是"
4. 检查：确保理解题意

### 简答题策略
1. 先列提纲，再展开
2. 分点作答，条理清晰
3. 适当举例说明
4. 画图辅助（如决策边界、网络结构）

### 计算题策略
1. 写出公式
2. 代入数值
3. 展示计算过程
4. 检查结果合理性
5. 标注单位（如果有）

### 综合题策略
1. 分析问题场景
2. 列举多个方案
3. 比较优缺点
4. 给出推荐理由
5. 考虑实际约束

---

## 📈 提分建议

### 如果基础薄弱 (目标60-70分)
**重点**：
- 核心概念理解
- 基本算法原理
- 常见公式记忆

**策略**：
- 专注PPT核心内容
- 记忆关键公式
- 做历年真题

### 如果基础中等 (目标70-85分)
**重点**：
- 算法深入理解
- 公式推导能力
- 应用题分析

**策略**：
- 手推重要公式
- 对比不同算法
- 练习综合题

### 如果基础较好 (目标85-100分)
**重点**：
- 理论深度理解
- 创新性思考
- 扩展知识

**策略**：
- 研究算法变体
- 阅读相关论文
- 思考实际应用

---

## ⏰ 考前最后24小时

### 最后一天晚上
- ✅ 快速浏览所有PPT
- ✅ 默写核心公式
- ✅ 复习错题集
- ❌ 不要学新知识
- ❌ 不要熬夜

### 考试当天早上
- ✅ 适度复习（30分钟）
- ✅ 吃早餐，补充能量
- ✅ 提前到达考场
- ✅ 准备好文具

### 考试中
- ✅ 先浏览全卷
- ✅ 先易后难
- ✅ 合理分配时间
- ✅ 预留检查时间
- ✅ 字迹工整

---

## 🌟 鼓励的话

> 复习不是临时抱佛脚，而是系统性地巩固知识。
> 
> 理解比记忆更重要，应用比理论更有价值。
> 
> 相信自己的准备，保持冷静的心态。

**你可以做到的！加油！** 💪🎓

---

*祝你考试顺利，取得优异成绩！* 🌟
